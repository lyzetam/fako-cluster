# infrastructure/controllers/base/cluster-diagnostics/cluster-recovery.yaml
# Emergency cluster recovery job to fix common issues

apiVersion: v1
kind: ServiceAccount
metadata:
  name: cluster-recovery-admin
  namespace: cluster-diagnostics
  labels:
    app.kubernetes.io/name: cluster-recovery
    app.kubernetes.io/part-of: cluster-diagnostics
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-recovery-admin
  labels:
    app.kubernetes.io/name: cluster-recovery
    app.kubernetes.io/part-of: cluster-diagnostics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: cluster-recovery-admin
  namespace: cluster-diagnostics
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: recovery-scripts
  namespace: cluster-diagnostics
  labels:
    app.kubernetes.io/name: cluster-recovery
    app.kubernetes.io/part-of: cluster-diagnostics
data:
  recover-cluster.sh: |
    #!/bin/bash
    set -e
    
    # Color codes
    RED='\033[0;31m'
    GREEN='\033[0;32m'
    YELLOW='\033[1;33m'
    BLUE='\033[0;34m'
    NC='\033[0m'
    
    # Helper functions
    log_info() {
      echo -e "${BLUE}[INFO]${NC} $1"
    }
    
    log_success() {
      echo -e "${GREEN}[SUCCESS]${NC} $1"
    }
    
    log_warning() {
      echo -e "${YELLOW}[WARNING]${NC} $1"
    }
    
    log_error() {
      echo -e "${RED}[ERROR]${NC} $1"
    }
    
    echo "=========================================="
    echo "   KUBERNETES CLUSTER RECOVERY TOOL"
    echo "=========================================="
    echo "Start time: $(date)"
    echo ""
    
    # Step 1: Clean up failed resources
    log_info "Step 1: Cleaning up failed and stuck resources..."
    
    # Delete failed pods
    failed_pods=$(kubectl get pods --all-namespaces --field-selector=status.phase=Failed -o json | jq -r '.items[] | "\(.metadata.namespace)/\(.metadata.name)"' 2>/dev/null || true)
    if [ -n "$failed_pods" ]; then
      log_warning "Removing failed pods..."
      for pod in $failed_pods; do
        namespace=$(echo $pod | cut -d'/' -f1)
        name=$(echo $pod | cut -d'/' -f2)
        kubectl delete pod $name -n $namespace --force --grace-period=0 2>/dev/null || true
      done
    fi
    
    # Delete terminating pods
    kubectl get pods --all-namespaces | grep Terminating | while read ns name rest; do
      kubectl delete pod $name -n $ns --force --grace-period=0 2>/dev/null || true
    done
    
    # Step 2: Restart critical components
    log_info "Step 2: Restarting critical system components..."
    
    kubectl rollout restart deployment -n kube-system coredns 2>/dev/null || true
    kubectl rollout restart daemonset -n kube-system kube-proxy 2>/dev/null || true
    
    # Step 3: Ensure namespaces exist
    log_info "Step 3: Ensuring required namespaces exist..."
    
    # From your kustomization files
    required_namespaces=(
      "flux-system"
      "monitoring"
      "nvidia-device-plugin"
      "nfs-system"
      "ollama"
      "whisper"
      "piper"
      "openwakeword"
      "voice-monitor"
      "audiobookshelf"
      "linkding"
      "homebot"
      "cluster-diagnostics"
      "cleanup-jobs"
      "renovate"
    )
    
    for ns in "${required_namespaces[@]}"; do
      kubectl create namespace $ns --dry-run=client -o yaml | kubectl apply -f - 2>/dev/null
    done
    
    # Step 4: Check Flux
    log_info "Step 4: Checking Flux GitOps..."
    
    if kubectl get namespace flux-system > /dev/null 2>&1; then
      # Force reconciliation
      kubectl -n flux-system get gitrepository -o name | while read repo; do
        kubectl annotate $repo -n flux-system reconcile.fluxcd.io/requestedAt="$(date +%s)" --overwrite
      done
      
      kubectl -n flux-system get kustomization -o name | while read ks; do
        kubectl annotate $ks -n flux-system reconcile.fluxcd.io/requestedAt="$(date +%s)" --overwrite
      done
    fi
    
    # Step 5: Storage check
    log_info "Step 5: Checking storage..."
    
    kubectl get storageclass
    kubectl get pvc --all-namespaces | grep -v Bound || true
    
    # Step 6: Node check
    log_info "Step 6: Checking nodes..."
    
    kubectl get nodes
    
    # Check for NotReady nodes
    not_ready=$(kubectl get nodes | grep -c NotReady || true)
    if [ "$not_ready" -gt 0 ]; then
      log_warning "$not_ready node(s) are NotReady"
      log_warning "Manual intervention required:"
      log_warning "1. SSH to nodes and check: systemctl status kubelet"
      log_warning "2. Check disk space: df -h"
      log_warning "3. Check container runtime: systemctl status containerd"
    fi
    
    echo ""
    echo "=========================================="
    echo "Recovery process completed"
    echo "End time: $(date)"
    echo "=========================================="
---
apiVersion: batch/v1
kind: Job
metadata:
  name: cluster-recovery
  namespace: cluster-diagnostics
  labels:
    app.kubernetes.io/name: cluster-recovery
    app.kubernetes.io/part-of: cluster-diagnostics
spec:
  ttlSecondsAfterFinished: 1800
  backoffLimit: 0
  activeDeadlineSeconds: 600
  
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cluster-recovery
        app.kubernetes.io/part-of: cluster-diagnostics
    spec:
      serviceAccountName: cluster-recovery-admin
      restartPolicy: Never
      
      tolerations:
      - operator: Exists
        
      containers:
      - name: recovery
        image: bitnami/kubectl:1.29
        imagePullPolicy: IfNotPresent
        
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "500m"
            
        command: ["/bin/bash", "/scripts/recover-cluster.sh"]
        
        volumeMounts:
        - name: scripts
          mountPath: /scripts
          readOnly: true
          
      volumes:
      - name: scripts
        configMap:
          name: recovery-scripts
          defaultMode: 0755
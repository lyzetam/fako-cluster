apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-gpu
  namespace: ollama
spec:
  replicas: 1
  template:
    spec:
      # Remove GPU node selector
      nodeSelector: {}
      
      # Remove GPU tolerations
      tolerations: []
      
      # Remove nvidia RuntimeClass
      runtimeClassName: null
      
      containers:
      - name: ollama
        # Remove GPU ConfigMap reference
        envFrom:
        - configMapRef:
            name: ollama-configmap
        # Override specific values for CPU mode
        env:
        - name: OLLAMA_GPU_LAYERS
          value: "0"  # Force CPU usage
        - name: OLLAMA_NUM_PARALLEL
          value: "1"  # Reduce parallel requests for CPU
        - name: OLLAMA_NUM_THREAD
          value: "4"  # Adjust based on available CPU cores
        - name: OLLAMA_MAX_LOADED_MODELS
          value: "1"  # Keep only one model loaded
        
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
            nvidia.com/gpu: null  # Remove GPU request
          limits:
            memory: "8Gi"
            cpu: "4000m"
            nvidia.com/gpu: null  # Remove GPU limit
      
      # Simplified model manager for CPU
      - name: model-manager
        args:
        - -c
        - |
          echo "Model manager starting (CPU mode)..."
          sleep 60
          
          export OLLAMA_HOST=http://localhost:11434
          
          echo "Downloading CPU-optimized models..."
          
          # Only small models suitable for CPU
          echo "=== CPU-OPTIMIZED MODELS ==="
          ollama pull tinyllama || echo "Failed tinyllama"
          ollama pull phi3:mini || echo "Failed phi3:mini"
          
          echo "Model download complete."
          ollama list
          
          while true; do
            sleep 600
            echo "Model manager heartbeat..."
          done

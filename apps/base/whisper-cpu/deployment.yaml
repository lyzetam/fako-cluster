# apps/base/whisper-cpu/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: whisper-cpu
  namespace: whisper-cpu
spec:
  replicas: 8  # Increased for parallel processing
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: whisper-cpu
  template:
    metadata:
      labels:
        app: whisper-cpu
    spec:
      # No imagePullSecrets needed - ghcr.io is public

      # Ensure consistent permissions across init and main containers
      securityContext:
        runAsUser: 1000      # ubuntu user in speaches image
        runAsGroup: 1000
        fsGroup: 1000        # Ensures PVC is writable by group

      # Init container to download models before main container starts
      initContainers:
      - name: model-downloader
        image: ghcr.io/speaches-ai/speaches:latest-cpu
        command:
        - /bin/sh
        - -c
        - |
          echo "Fixing permissions on existing cache..."
          chmod -R u+rwX /home/ubuntu/.cache/huggingface/hub 2>/dev/null || true
          echo "Downloading Whisper model..."
          python3 -c "
          from huggingface_hub import snapshot_download
          model_id = 'Systran/faster-distil-whisper-large-v3'
          print(f'Downloading {model_id}...')
          snapshot_download(repo_id=model_id, repo_type='model')
          print(f'Successfully downloaded {model_id}')
          "
          echo "Model download complete"
        env:
        - name: HF_HOME
          value: "/home/ubuntu/.cache/huggingface"
        volumeMounts:
        - mountPath: /home/ubuntu/.cache/huggingface/hub
          name: whisper-models
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"

      containers:
      - name: whisper
        image: ghcr.io/speaches-ai/speaches:latest-cpu
        ports:
        - containerPort: 8000
          protocol: TCP
        env:
        - name: WHISPER__INFERENCE_DEVICE
          value: "cpu"
        - name: WHISPER__COMPUTE_TYPE
          value: "int8"
        - name: HF_HOME
          value: "/home/ubuntu/.cache/huggingface"
        # Keep models loaded longer (10 min instead of 5 min default)
        - name: STT_MODEL_TTL
          value: "600"

        volumeMounts:
        - mountPath: /home/ubuntu/.cache/huggingface/hub
          name: whisper-models

        resources:
          requests:
            memory: "2Gi"    # Actual usage: 150Mi-1.5Gi
            cpu: "1000m"     # Actual usage: ~5m idle, spikes during inference
          limits:
            memory: "4Gi"
            cpu: "4000m"

        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60  # Models already downloaded by init
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30  # Faster startup since models cached
          periodSeconds: 10
          timeoutSeconds: 5

      volumes:
      - name: whisper-models
        persistentVolumeClaim:
          claimName: whisper-cpu-models

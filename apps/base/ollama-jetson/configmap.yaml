apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-configmap
  namespace: ollama-jetson
data:
  OLLAMA_HOST: "0.0.0.0"
  OLLAMA_ORIGINS: "*"
  OLLAMA_MODELS: "/root/.ollama/models"
  OLLAMA_DEBUG: "false"
  OLLAMA_NOHISTORY: "false"  # Keep conversation history
  
  # ARM-specific optimizations for Jetson
  GOARCH: "arm64"
  GOOS: "linux"
---
# Separate ConfigMap for GPU-specific settings (Jetson Orin Nano)
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-gpu-configmap
  namespace: ollama-jetson
data:
  # GPU Configuration for Jetson
  CUDA_VISIBLE_DEVICES: "0"
  NVIDIA_VISIBLE_DEVICES: "0"
  NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
  
  # Ollama GPU Settings - optimized for Jetson Orin Nano
  # OLLAMA_GPU_LAYERS: "999"  # Original: Force all layers to GPU
  OLLAMA_GPU_LAYERS: "26"  # Optimized: Fixed at 26 layers for stable performance
  OLLAMA_CUDA_FORCE_MMQ: "1"
  OLLAMA_FLASH_ATTENTION: "0"  # Disabled - not supported
  # OLLAMA_GPU_MEMORY: "6G"  # Original: Conservative for 8GB system
  OLLAMA_GPU_MEMORY: "5G"  # Optimized: More conservative to prevent OOM
  
  # Performance Settings - adjusted for ARM Cortex-A78AE
  OLLAMA_NUM_PARALLEL: "1"  # Conservative for Jetson
  OLLAMA_NUM_THREAD: "6"    # 6-core CPU
  OLLAMA_MAX_LOADED_MODELS: "1"  # Keep one model loaded
  GOMAXPROCS: "6"
  
  # Memory Management - optimized for voice pipeline
  OLLAMA_KEEP_ALIVE: "24h"  # Keep model loaded for 24 hours to prevent cold starts
  OLLAMA_NO_MMAP: "false"   # Keep memory mapping for efficiency
  
  # Force CUDA path
  OLLAMA_LLM_LIBRARY: "cublas"
  
  # Context and batch settings - optimized for 8GB Orin
  # OLLAMA_CONTEXT_LENGTH: "8192"  # Original: Large context
  OLLAMA_CONTEXT_LENGTH: "12000"  # Increased context for longer conversations
  # OLLAMA_CONTEXT_LENGTH: "20000"  # Increased context for longer conversations
  
  # Additional settings for voice pipeline optimization
  OLLAMA_BATCH_SIZE: "128"  # Reduced from default 512 for lower latency
  OLLAMA_MAX_QUEUE: "1"     # Prevent request queuing
  
  # GPU overhead - use all available VRAM since dedicated device
  OLLAMA_GPU_OVERHEAD: "0"

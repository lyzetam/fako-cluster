apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-configmap
  namespace: ollama-jetson
data:
  OLLAMA_HOST: "0.0.0.0"
  OLLAMA_ORIGINS: "*"
  OLLAMA_MODELS: "/root/.ollama/models"
  OLLAMA_DEBUG: "false"
  OLLAMA_NOHISTORY: "false"  # Keep conversation history
  
  # ARM-specific optimizations for Jetson
  GOARCH: "arm64"
  GOOS: "linux"
---
# Separate ConfigMap for GPU-specific settings (Jetson Orin Nano)
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-gpu-configmap
  namespace: ollama-jetson
data:
  # GPU Configuration for Jetson
  CUDA_VISIBLE_DEVICES: "0"
  NVIDIA_VISIBLE_DEVICES: "0"
  NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
  
  # Ollama GPU Settings - optimized for Jetson Orin Nano with 2-3B models
  OLLAMA_GPU_LAYERS: "999"  # For 2-3B models, can fit all layers in GPU
  OLLAMA_CUDA_FORCE_MMQ: "1"
  OLLAMA_FLASH_ATTENTION: "0"  # Disabled - not supported
  OLLAMA_GPU_MEMORY: "4G"  # Reduced for smaller 2-3B models
  
  # Performance Settings - adjusted for ARM Cortex-A78AE
  OLLAMA_NUM_PARALLEL: "1"  # Conservative for Jetson
  OLLAMA_NUM_THREAD: "6"    # 6-core CPU
  OLLAMA_MAX_LOADED_MODELS: "1"  # Keep one model loaded
  GOMAXPROCS: "6"
  
  # Memory Management - optimized for voice pipeline
  OLLAMA_KEEP_ALIVE: "24h"  # Keep model loaded for 24 hours to prevent cold starts
  OLLAMA_NO_MMAP: "false"   # Keep memory mapping for efficiency
  
  # Force CUDA path
  OLLAMA_LLM_LIBRARY: "cublas"
  
  # Context and batch settings - optimized for 2-3B models on 8GB Orin
  OLLAMA_CONTEXT_LENGTH: "4096"  # Reduced for better performance with smaller models
  
  # Additional settings for voice pipeline optimization with smaller models
  OLLAMA_BATCH_SIZE: "256"  # Increased slightly as smaller models can handle more
  OLLAMA_MAX_QUEUE: "1"     # Prevent request queuing
  
  # GPU overhead - use all available VRAM since dedicated device
  OLLAMA_GPU_OVERHEAD: "0"

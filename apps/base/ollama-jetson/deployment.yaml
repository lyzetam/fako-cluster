apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-jetson
  namespace: ollama-jetson
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: ollama-jetson
  template:
    metadata:
      labels:
        app: ollama-jetson
    spec:
      securityContext:
        fsGroup: 1000
      # Target the jetson01 node
      nodeSelector:
        kubernetes.io/hostname: jetson01
      
      # Use NVIDIA runtime for GPU support
      runtimeClassName: nvidia
      
      # No special tolerations needed for Jetson
      tolerations: []

      # Enable host memory access for large models
      hostIPC: true
      
      containers:
      - name: ollama
        # Use latest ollama image which has better Jetson support
        image: ollama/ollama:latest
        ports:
        - containerPort: 11434
          protocol: TCP
        envFrom:
        - configMapRef:
            name: ollama-configmap
        - configMapRef:
            name: ollama-gpu-configmap
        
        volumeMounts:
        - mountPath: /root/.ollama
          name: ollama-jetson-models
        
        resources:
          requests:
            memory: "2Gi"     # Conservative start
            cpu: "2000m"      # 2 cores
          limits:
            nvidia.com/gpu: 1 # Request 1 GPU
            memory: "6Gi"     # Leave 2GB for system
            cpu: "5000m"      # 5 of 6 cores
        
        livenessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
      
      # Model manager sidecar - optimized for Jetson
      - name: model-manager
        image: ollama/ollama:latest
        command: ["/bin/sh"]
        args:
        - -c
        - |
          echo "Model manager starting for Jetson Orin Nano..."
          sleep 60
          
          export OLLAMA_HOST=http://localhost:11434
          
          echo "Loading qwen2.5:7b into memory..."
          ollama run qwen2.5:7b "test" || echo "Failed to load qwen2.5:7b"
          
          echo "Downloading additional models (not loading into memory)..."
          
          # Small efficient models
          echo "=== SMALL EFFICIENT MODELS ==="
          ollama pull qwen2.5:0.5b || echo "Failed qwen2.5:0.5b"
          ollama pull llama3.2:3b || echo "Failed llama3.2:3b"
          
          # Medium models - good for Jetson
          echo "=== MEDIUM MODELS ==="
          ollama pull qwen2.5:7b || echo "Failed qwen2.5:7b"
          ollama pull llama3.1:8b || echo "Failed llama3.1:8b"
          ollama pull deepseek-coder:6.7b-instruct || echo "Failed deepseek-coder:6.7b-instruct"
          ollama pull qwen3:8b || echo "Failed qwen3:8b"
          
          # Larger models that can fit with quantization
          echo "=== LARGER MODELS ==="
          ollama pull deepseek-r1:14b || echo "Failed deepseek-r1:14b"
          ollama pull ishumilin/deepseek-r1-coder-tools:14b || echo "Failed deepseek-r1-coder-tools:14b"
          
          echo "Model setup complete."
          ollama list
          
          # Keep qwen2.5:7b loaded in memory
          while true; do
            sleep 600
            echo "Model manager heartbeat..."
            # Periodically ensure qwen2.5:7b stays loaded
            ollama run qwen2.5:7b "ping" > /dev/null 2>&1 || echo "Re-loading qwen2.5:7b"
          done
        
        volumeMounts:
        - mountPath: /root/.ollama
          name: ollama-jetson-models
        resources:
          requests:
            memory: "500Mi"  
            cpu: "250m"     
          limits:
            memory: "1Gi"
            cpu: "500m"
      
      volumes:
      - name: ollama-jetson-models
        persistentVolumeClaim:
          claimName: ollama-jetson-models

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-jetson
  namespace: ollama-jetson
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: ollama-jetson
  template:
    metadata:
      labels:
        app: ollama-jetson
    spec:
      securityContext:
        fsGroup: 1000
      # Target the jetson01 node
      nodeSelector:
        kubernetes.io/hostname: jetson01
      
      # Use NVIDIA runtime for GPU support
      runtimeClassName: nvidia
      
      # No special tolerations needed for Jetson
      tolerations: []

      # Enable host memory access for large models
      hostIPC: true
      
      containers:
      - name: ollama
        # Use latest ollama image which has better Jetson support
        image: ollama/ollama:latest
        args: ["serve", "--batch-size", "256"]
        ports:
        - containerPort: 11434
          protocol: TCP
        envFrom:
        - configMapRef:
            name: ollama-configmap
        - configMapRef:
            name: ollama-gpu-configmap
        
        volumeMounts:
        - mountPath: /root/.ollama
          name: ollama-jetson-models
        
        resources:
          requests:
            memory: "2Gi"     # Conservative start
            cpu: "2000m"      # 2 cores
          limits:
            nvidia.com/gpu: 1 # Request 1 GPU
            memory: "7Gi"     # Use more memory since dedicated
            cpu: "6000m"      # Use all 6 cores
        
        livenessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
      
      # Model manager sidecar - optimized for Jetson
      - name: model-manager
        image: ollama/ollama:latest
        command: ["/bin/sh"]
        args:
        - -c
        - |
          echo "Model manager starting for dedicated Jetson Orin Nano..."
          sleep 60
          
          export OLLAMA_HOST=http://localhost:11434
          
          # Ensure qwen2.5:7b is downloaded
          echo "Pulling qwen2.5:7b model..."
          ollama pull qwen2.5:7b || echo "Failed to pull qwen2.5:7b"
          
          # Load qwen2.5:7b into memory
          echo "Loading qwen2.5:7b into memory..."
          ollama run qwen2.5:7b "Initialize model" || echo "Failed to load qwen2.5:7b"
          
          echo "Model setup complete. Only qwen2.5:7b is loaded."
          ollama list
          
          # Keep qwen2.5:7b loaded in memory permanently
          while true; do
            sleep 600
            echo "Model manager heartbeat - keeping qwen2.5:7b loaded..."
            # Ensure qwen2.5:7b stays loaded
            ollama run qwen2.5:7b "keepalive" > /dev/null 2>&1 || echo "Re-loading qwen2.5:7b"
          done
        
        volumeMounts:
        - mountPath: /root/.ollama
          name: ollama-jetson-models
        resources:
          requests:
            memory: "500Mi"  
            cpu: "250m"     
          limits:
            memory: "1Gi"
            cpu: "500m"
      
      volumes:
      - name: ollama-jetson-models
        persistentVolumeClaim:
          claimName: ollama-jetson-models

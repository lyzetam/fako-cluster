apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-jetson
  namespace: ollama-jetson
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: ollama-jetson
  template:
    metadata:
      labels:
        app: ollama-jetson
    spec:
      securityContext:
        fsGroup: 1000
      # Target the jetson01 node
      nodeSelector:
        kubernetes.io/hostname: jetson01
      
      # Use NVIDIA runtime for GPU support
      runtimeClassName: nvidia
      
      # No special tolerations needed for Jetson
      tolerations: []

      # Enable host memory access for large models
      hostIPC: true
      
      containers:
      - name: ollama
        # Use latest ollama image which has better Jetson support
        image: ollama/ollama:latest
        ports:
        - containerPort: 11434
          protocol: TCP
        envFrom:
        - configMapRef:
            name: ollama-configmap
        - configMapRef:
            name: ollama-gpu-configmap
        
        volumeMounts:
        - mountPath: /root/.ollama
          name: ollama-jetson-models
        
        resources:
          requests:
            memory: "2Gi"     # Conservative start
            cpu: "2000m"      # 2 cores
          limits:
            nvidia.com/gpu: 1 # Request 1 GPU
            memory: "7Gi"     # Use more memory since dedicated
            cpu: "6000m"      # Use all 6 cores
        
        # Add startup probe for model loading
        startupProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30  # Allow up to 5 minutes for initial model load
        
        livenessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
      
      # Model manager sidecar - optimized for voice pipeline
      - name: model-manager
        image: ollama/ollama:latest
        command: ["/bin/sh"]
        args:
        - -c
        - |
          echo "Voice-optimized model manager starting..."
          sleep 60
          
          export OLLAMA_HOST=http://localhost:11434
          
          # Ensure qwen2.5:7b is downloaded
          echo "Pulling qwen2.5:7b model..."
          ollama pull qwen2.5:7b || echo "Failed to pull qwen2.5:7b"
          
          # Create voice-optimized variant with proper settings
          echo "Creating voice-optimized model variant..."
          cat > /tmp/qwen-voice.modelfile <<EOF
          FROM qwen2.5:7b
          
          # Model parameters optimized for voice pipeline
          PARAMETER num_ctx 20000
          PARAMETER num_batch 128
          PARAMETER num_gpu 26
          PARAMETER temperature 0.3
          PARAMETER top_p 0.9
          PARAMETER repeat_penalty 1.1
          
          # System prompt for voice assistant
          SYSTEM You are a helpful voice assistant. Keep responses concise and clear.
          EOF
          
          ollama create qwen2.5-voice -f /tmp/qwen-voice.modelfile
          
          # Pre-load the model with optimized settings
          echo "Pre-loading voice model..."
          ollama run qwen2.5-voice "Initialize voice assistant" --verbose
          
          echo "Model setup complete. Voice-optimized model is loaded."
          ollama list
          
          # Keep model warm with more frequent pings (5 minutes)
          while true; do
            sleep 300  # 5 minutes instead of 10
            echo "Keeping voice model warm..."
            # Use minimal prompt to keep model loaded
            ollama run qwen2.5-voice "ping" > /dev/null 2>&1 || {
              echo "Model needs reloading..."
              ollama run qwen2.5-voice "Initialize" > /dev/null 2>&1
            }
          done
        
        volumeMounts:
        - mountPath: /root/.ollama
          name: ollama-jetson-models
        resources:
          requests:
            memory: "500Mi"  
            cpu: "250m"     
          limits:
            memory: "1Gi"
            cpu: "500m"
      
      volumes:
      - name: ollama-jetson-models
        persistentVolumeClaim:
          claimName: ollama-jetson-models

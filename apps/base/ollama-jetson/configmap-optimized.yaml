apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-configmap
  namespace: ollama-jetson
data:
  OLLAMA_HOST: "0.0.0.0"
  OLLAMA_ORIGINS: "*"
  OLLAMA_MODELS: "/root/.ollama/models"
  OLLAMA_DEBUG: "false"
  OLLAMA_NOHISTORY: "false"  # Keep conversation history
  
  # ARM-specific optimizations for Jetson
  GOARCH: "arm64"
  GOOS: "linux"
---
# Optimized ConfigMap for GPU-specific settings (Jetson Orin Nano)
# Based on performance analysis showing 51s response time and high memory usage
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-gpu-configmap
  namespace: ollama-jetson
data:
  # GPU Configuration for Jetson
  CUDA_VISIBLE_DEVICES: "0"
  NVIDIA_VISIBLE_DEVICES: "0"
  NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
  
  # Keep all layers on GPU since this is a dedicated node
  OLLAMA_GPU_LAYERS: "999"  # All layers on GPU for maximum performance
  OLLAMA_CUDA_FORCE_MMQ: "1"
  OLLAMA_FLASH_ATTENTION: "0"  # Disabled - not supported
  
  # GPU memory with headroom for system
  OLLAMA_GPU_MEMORY: "4.5G"  # Reserve 0.5G for system overhead
  
  # Performance Settings - reserve 1 core for system
  OLLAMA_NUM_PARALLEL: "1"  # Keep conservative for Jetson
  OLLAMA_NUM_THREAD: "5"    # Use 5 of 6 cores, reserve 1 for system
  OLLAMA_MAX_LOADED_MODELS: "1"  # Keep one model loaded
  GOMAXPROCS: "5"  # Reserve 1 core for system processes
  
  # Keep model in memory permanently since dedicated node
  OLLAMA_KEEP_ALIVE: "24h"  # Keep loaded for 24 hours on dedicated node
  OLLAMA_NO_MMAP: "false"  # Keep memory mapping for efficiency
  
  # Force CUDA path
  OLLAMA_LLM_LIBRARY: "cublas"
  
  # OPTIMIZATION 4: Reduce context for faster inference
  # Logs show n_ctx=8192 but this contributes to 51s response time
  OLLAMA_CONTEXT_LENGTH: "4096"  # Reduced for faster responses
  
  # OPTIMIZATION 5: Reduce batch size for lower latency
  # Logs show n_batch=512 despite config saying 256
  OLLAMA_BATCH_SIZE: "128"  # Reduced from 256 for lower latency
  OLLAMA_MAX_QUEUE: "1"     # Prevent request queuing
  
  # GPU overhead - use all available VRAM since dedicated device
  OLLAMA_GPU_OVERHEAD: "0"
  
  # Additional optimizations for inference speed
  OLLAMA_NUM_GPU: "1"       # Explicitly set single GPU
  OLLAMA_LOW_VRAM: "false"  # We have enough VRAM for 2-3B models

apiVersion: v1
kind: ConfigMap
metadata:
  name: transcript-publisher-script
  namespace: audio-workflows
data:
  publisher.py: |
    #!/usr/bin/env python3
    """
    Transcript Publisher - Posts raw transcripts to Obsidian vault
    Modified for Argo Workflows with timestamp-based filtering
    """
    import os
    import json
    import logging
    import textwrap
    import ssl
    import urllib.request
    import urllib.error
    from pathlib import Path
    from datetime import datetime
    import time

    # Create SSL context that doesn't verify certificates (for self-signed certs)
    SSL_CONTEXT = ssl.create_default_context()
    SSL_CONTEXT.check_hostname = False
    SSL_CONTEXT.verify_mode = ssl.CERT_NONE

    # Configuration from environment
    INPUT_DIR = os.environ.get('INPUT_DIR', '/data/transcriptions')
    STATE_DIR = os.environ.get('STATE_DIR', '/data/state')
    OBSIDIAN_FOLDER = os.environ.get('OBSIDIAN_FOLDER', 'Transcripts/Raw')
    OBSIDIAN_API_URL = os.environ['OBSIDIAN_API_URL']
    OBSIDIAN_API_KEY = os.environ['OBSIDIAN_API_KEY']
    FILE_PREFIX = os.environ.get('FILE_PREFIX', 'Transcript')
    MAX_RETRIES = int(os.environ.get('MAX_RETRIES', '3'))
    RETRY_DELAY = int(os.environ.get('RETRY_DELAY', '10'))
    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO')

    # Timestamp-based filtering (for Argo Workflows)
    USE_TIMESTAMP_FILTER = os.environ.get('USE_TIMESTAMP_FILTER', 'false').lower() == 'true'
    WORKFLOW_START_TIME = os.environ.get('WORKFLOW_START_TIME', '')
    # Legacy state file-based filtering
    SKIP_PROCESSED = os.environ.get('SKIP_PROCESSED', 'true').lower() == 'true'

    logging.basicConfig(level=getattr(logging, LOG_LEVEL),
                        format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)

    STATE_FILE = Path(STATE_DIR) / 'publisher-processed.txt'

    def load_processed():
        if STATE_FILE.exists():
            return set(STATE_FILE.read_text().strip().split('\n'))
        return set()

    def save_processed(processed):
        STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
        STATE_FILE.write_text('\n'.join(sorted(processed)))

    def get_workflow_cutoff_time():
        """Parse workflow start time for timestamp filtering."""
        if WORKFLOW_START_TIME:
            try:
                # Support ISO format from Argo
                return datetime.fromisoformat(WORKFLOW_START_TIME.replace('Z', '+00:00'))
            except ValueError:
                logger.warning(f"Could not parse WORKFLOW_START_TIME: {WORKFLOW_START_TIME}")
        return None

    def should_process_file(json_file, processed_set, cutoff_time):
        """Determine if file should be processed based on filtering mode."""
        if USE_TIMESTAMP_FILTER and cutoff_time:
            # Use file modification time for filtering (timezone-aware to match cutoff_time)
            from datetime import timezone
            mtime = datetime.fromtimestamp(json_file.stat().st_mtime, tz=timezone.utc)
            # Process files modified since last workflow (with buffer)
            if mtime < cutoff_time:
                logger.debug(f"Skipping {json_file.name} - older than cutoff")
                return False
            return True
        elif SKIP_PROCESSED:
            # Legacy state file-based filtering
            if json_file.name in processed_set:
                logger.debug(f"Skipping already processed: {json_file.name}")
                return False
            return True
        return True

    def json_to_markdown(data, filename):
        """Convert Whisper JSON transcript to readable Markdown."""
        text = data.get('text', '')
        duration = data.get('duration', 0)
        language = data.get('language', 'unknown')

        # Format duration
        mins, secs = divmod(int(duration), 60)
        hours, mins = divmod(mins, 60)
        duration_str = f"{hours}h {mins}m {secs}s" if hours else f"{mins}m {secs}s"

        md = textwrap.dedent(f"""\
            # {FILE_PREFIX}: {filename}

            **Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M')}
            **Duration**: {duration_str}
            **Language**: {language}

            ---

            ## Full Transcript

            {text}

            ---

            *Published by transcript-publisher*
            """)
        return md

    def post_to_obsidian(filename, content):
        """Post markdown content to Obsidian REST API."""
        url = f"{OBSIDIAN_API_URL}/vault/{OBSIDIAN_FOLDER}/{filename}"
        headers = {
            'Authorization': f'Bearer {OBSIDIAN_API_KEY}',
            'Content-Type': 'text/markdown'
        }

        for attempt in range(MAX_RETRIES):
            try:
                req = urllib.request.Request(url, data=content.encode('utf-8'), headers=headers, method='PUT')
                with urllib.request.urlopen(req, context=SSL_CONTEXT) as response:
                    if response.status in (200, 201, 204):
                        logger.info(f"Posted {filename} to Obsidian")
                        return True
                    logger.warning(f"Obsidian API returned {response.status}")
            except urllib.error.HTTPError as e:
                logger.warning(f"Obsidian API returned {e.code}: {e.reason}")
            except Exception as e:
                logger.error(f"Attempt {attempt+1} failed: {e}")

            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)

        return False

    def main():
        logger.info(f"Starting transcript publisher")
        logger.info(f"Input: {INPUT_DIR}, Output: {OBSIDIAN_FOLDER}")
        logger.info(f"Filtering mode: {'timestamp' if USE_TIMESTAMP_FILTER else 'state-file'}")

        processed = load_processed() if SKIP_PROCESSED and not USE_TIMESTAMP_FILTER else set()
        cutoff_time = get_workflow_cutoff_time()
        input_path = Path(INPUT_DIR)

        json_files = list(input_path.glob('*.json'))
        logger.info(f"Found {len(json_files)} JSON files")

        success_count = 0
        for json_file in json_files:
            if not should_process_file(json_file, processed, cutoff_time):
                continue

            try:
                data = json.loads(json_file.read_text())
                md_content = json_to_markdown(data, json_file.stem)
                md_filename = f"{json_file.stem}.md"

                if post_to_obsidian(md_filename, md_content):
                    processed.add(json_file.name)
                    success_count += 1
            except Exception as e:
                logger.error(f"Failed to process {json_file.name}: {e}")

        if SKIP_PROCESSED and not USE_TIMESTAMP_FILTER:
            save_processed(processed)
        logger.info(f"Published {success_count} transcripts to Obsidian")

    if __name__ == '__main__':
        main()

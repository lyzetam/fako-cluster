apiVersion: v1
kind: ConfigMap
metadata:
  name: daily-diary-script
  namespace: audio-workflows
data:
  daily-diary.py: |
    #!/usr/bin/env python3
    """
    Daily Diary Aggregator - Combines all transcripts from the same date into a
    narrative "Day in the Life" summary and posts to Obsidian.
    Modified for Argo Workflows with timestamp-based filtering
    """
    import os
    import json
    import logging
    import textwrap
    import ssl
    import urllib.request
    import urllib.error
    import re
    from pathlib import Path
    from datetime import datetime
    from collections import defaultdict
    import time

    # Create SSL context that doesn't verify certificates (for self-signed certs)
    SSL_CONTEXT = ssl.create_default_context()
    SSL_CONTEXT.check_hostname = False
    SSL_CONTEXT.verify_mode = ssl.CERT_NONE

    # Configuration from environment
    INPUT_DIR = os.environ.get('INPUT_DIR', '/data/transcriptions')
    STATE_DIR = os.environ.get('STATE_DIR', '/data/state')
    OBSIDIAN_FOLDER = os.environ.get('OBSIDIAN_FOLDER', 'Landry/daily-life')
    OBSIDIAN_API_URL = os.environ['OBSIDIAN_API_URL']
    OBSIDIAN_API_KEY = os.environ['OBSIDIAN_API_KEY']
    SUMMARIZER_BASE_URL = os.environ['SUMMARIZER_BASE_URL']
    SUMMARIZER_API_KEY = os.environ['SUMMARIZER_API_KEY']
    SUMMARIZER_MODEL = os.environ.get('SUMMARIZER_MODEL', 'gemma3')
    MAX_RETRIES = int(os.environ.get('MAX_RETRIES', '3'))
    RETRY_DELAY = int(os.environ.get('RETRY_DELAY', '10'))
    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO')
    MAX_TRANSCRIPT_CHARS = int(os.environ.get('MAX_TRANSCRIPT_CHARS', '50000'))

    # Timestamp-based filtering (for Argo Workflows)
    USE_TIMESTAMP_FILTER = os.environ.get('USE_TIMESTAMP_FILTER', 'false').lower() == 'true'
    WORKFLOW_START_TIME = os.environ.get('WORKFLOW_START_TIME', '')
    # Legacy state file-based filtering
    SKIP_PROCESSED = os.environ.get('SKIP_PROCESSED', 'true').lower() == 'true'

    DIARY_PROMPT = os.environ.get('DIARY_PROMPT', textwrap.dedent("""\
        You are creating a personal diary entry from audio transcripts recorded throughout the day.
        Write in a narrative "Day in the Life" style that captures the essence of conversations,
        activities, and thoughts from the day.

        Format the entry as follows:
        1. Start with a title: "# Day in the Life: [Full Date]"
        2. Break into sections with ## headers for major topics/conversations
        3. Use ### subheaders for details within sections
        4. Include relevant quotes using > blockquote format
        5. Use bullet points for lists of items discussed
        6. Add narrative context and flow between sections
        7. End with a brief reflection or key takeaway if appropriate
        8. Add a closing line like "---\\n*Transcribed from daily recordings*"

        Make it personal and readable - this is a life diary, not a business report.
        Capture the human moments, decisions, and conversations that made up the day.
        """))

    logging.basicConfig(level=getattr(logging, LOG_LEVEL),
                        format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)

    STATE_FILE = Path(STATE_DIR) / 'daily-diary-processed.txt'

    def load_processed():
        if STATE_FILE.exists():
            return set(STATE_FILE.read_text().strip().split('\n'))
        return set()

    def save_processed(processed):
        STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
        STATE_FILE.write_text('\n'.join(sorted(processed)))

    def get_workflow_cutoff_time():
        """Parse workflow start time for timestamp filtering."""
        if WORKFLOW_START_TIME:
            try:
                return datetime.fromisoformat(WORKFLOW_START_TIME.replace('Z', '+00:00'))
            except ValueError:
                logger.warning(f"Could not parse WORKFLOW_START_TIME: {WORKFLOW_START_TIME}")
        return None

    def extract_date_from_filename(filename):
        """Extract date from filename like '10-16-25' (MM-DD-YY) or '10-16-25-01'."""
        match = re.match(r'^(\d{2})-(\d{2})-(\d{2})', filename)
        if match:
            mm, dd, yy = match.groups()
            year = 2000 + int(yy)
            return f"{year}-{mm}-{dd}"
        return None

    def group_transcripts_by_date(json_files, cutoff_time=None):
        """Group JSON transcript files by date, optionally filtering by timestamp."""
        by_date = defaultdict(list)
        for json_file in json_files:
            # Apply timestamp filter if enabled
            if USE_TIMESTAMP_FILTER and cutoff_time:
                mtime = datetime.fromtimestamp(json_file.stat().st_mtime)
                if mtime < cutoff_time:
                    continue

            date_str = extract_date_from_filename(json_file.stem)
            if date_str:
                by_date[date_str].append(json_file)
            else:
                logger.warning(f"Could not extract date from: {json_file.name}")
        return by_date

    def combine_transcripts(json_files):
        """Combine multiple transcript files into one text."""
        combined_parts = []
        total_duration = 0

        for json_file in sorted(json_files, key=lambda f: f.name):
            try:
                data = json.loads(json_file.read_text())
                text = data.get('text', '')
                duration = data.get('duration', 0)

                if text:
                    if combined_parts:
                        combined_parts.append("\n\n--- Recording Break ---\n\n")
                    combined_parts.append(text)
                    total_duration += duration
            except Exception as e:
                logger.error(f"Failed to read {json_file.name}: {e}")

        return ''.join(combined_parts), total_duration

    def call_llm(transcript_text, date_str):
        """Call LLM to create diary entry from combined transcripts."""
        url = f"{SUMMARIZER_BASE_URL}/chat/completions"
        headers = {
            'Authorization': f'Bearer {SUMMARIZER_API_KEY}',
            'Content-Type': 'application/json'
        }

        if len(transcript_text) > MAX_TRANSCRIPT_CHARS:
            logger.warning(f"Truncating transcript from {len(transcript_text)} to {MAX_TRANSCRIPT_CHARS} chars")
            transcript_text = transcript_text[:MAX_TRANSCRIPT_CHARS] + "\n\n[... transcript truncated ...]"

        try:
            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
            formatted_date = date_obj.strftime('%B %d, %Y')
        except:
            formatted_date = date_str

        payload = {
            "model": SUMMARIZER_MODEL,
            "messages": [
                {"role": "system", "content": DIARY_PROMPT},
                {"role": "user", "content": f"Create a 'Day in the Life' diary entry for {formatted_date} from these transcripts:\n\n{transcript_text}"}
            ],
            "temperature": 0.5,
            "max_tokens": 4000
        }

        for attempt in range(MAX_RETRIES):
            try:
                data = json.dumps(payload).encode('utf-8')
                req = urllib.request.Request(url, data=data, headers=headers, method='POST')
                with urllib.request.urlopen(req, timeout=180) as response:
                    result = json.loads(response.read().decode('utf-8'))
                    return result['choices'][0]['message']['content']
            except urllib.error.HTTPError as e:
                error_body = e.read().decode('utf-8') if e.fp else ''
                logger.warning(f"LLM API returned {e.code}: {e.reason} - {error_body}")
            except Exception as e:
                logger.error(f"LLM attempt {attempt+1} failed: {e}")

            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)

        return None

    def post_to_obsidian(filename, content):
        """Post markdown content to Obsidian REST API."""
        url = f"{OBSIDIAN_API_URL}/vault/{OBSIDIAN_FOLDER}/{filename}"
        headers = {
            'Authorization': f'Bearer {OBSIDIAN_API_KEY}',
            'Content-Type': 'text/markdown'
        }

        for attempt in range(MAX_RETRIES):
            try:
                req = urllib.request.Request(url, data=content.encode('utf-8'), headers=headers, method='PUT')
                with urllib.request.urlopen(req, context=SSL_CONTEXT) as response:
                    if response.status in (200, 201, 204):
                        logger.info(f"Posted {filename} to Obsidian")
                        return True
                    logger.warning(f"Obsidian API returned {response.status}")
            except urllib.error.HTTPError as e:
                logger.warning(f"Obsidian API returned {e.code}: {e.reason}")
            except Exception as e:
                logger.error(f"Obsidian attempt {attempt+1} failed: {e}")

            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)

        return False

    def main():
        logger.info("Starting daily diary aggregator")
        logger.info(f"Input: {INPUT_DIR}, Output: {OBSIDIAN_FOLDER}")
        logger.info(f"Using model: {SUMMARIZER_MODEL}")
        logger.info(f"Filtering mode: {'timestamp' if USE_TIMESTAMP_FILTER else 'state-file'}")

        processed = load_processed() if SKIP_PROCESSED and not USE_TIMESTAMP_FILTER else set()
        cutoff_time = get_workflow_cutoff_time()
        input_path = Path(INPUT_DIR)

        json_files = list(input_path.glob('*.json'))
        logger.info(f"Found {len(json_files)} JSON files")

        # Group by date (with optional timestamp filtering)
        by_date = group_transcripts_by_date(json_files, cutoff_time)
        logger.info(f"Found {len(by_date)} unique dates to process")

        success_count = 0
        for date_str, files in sorted(by_date.items()):
            # Check if this date already processed (legacy mode)
            if SKIP_PROCESSED and not USE_TIMESTAMP_FILTER and date_str in processed:
                logger.debug(f"Skipping already processed date: {date_str}")
                continue

            try:
                logger.info(f"Processing date: {date_str} ({len(files)} recordings)")

                combined_text, total_duration = combine_transcripts(files)
                if not combined_text:
                    logger.warning(f"No transcript text for {date_str}")
                    continue

                diary_entry = call_llm(combined_text, date_str)
                if not diary_entry:
                    logger.error(f"Failed to generate diary for {date_str}")
                    continue

                md_filename = f"{date_str}.md"
                if post_to_obsidian(md_filename, diary_entry):
                    processed.add(date_str)
                    success_count += 1
                    logger.info(f"Successfully created diary for: {date_str}")

            except Exception as e:
                logger.error(f"Failed to process date {date_str}: {e}")

        if SKIP_PROCESSED and not USE_TIMESTAMP_FILTER:
            save_processed(processed)
        logger.info(f"Created {success_count} daily diary entries")

    if __name__ == '__main__':
        main()

# # apps/base/ollama/ollama-proxy-fixed.yaml
# apiVersion: v1
# kind: ConfigMap
# metadata:
#   name: ollama-proxy-config
#   namespace: ollama
# data:
#   nginx.conf: |
#     worker_processes 1;
#     error_log /dev/stderr debug;  # Enable debug logging
    
#     events {
#       worker_connections 1024;
#     }
    
#     http {
#       access_log /dev/stdout;
      
#       upstream exo_backend {
#         server 10.85.30.220:52415;
#       }
      
#       # Log format to debug requests
#       log_format detailed '$remote_addr - $remote_user [$time_local] '
#                          '"$request" $status $body_bytes_sent '
#                          '"$http_user_agent" "$request_body"';
      
#       server {
#         listen 11434;
#         access_log /dev/stdout detailed;
        
#         # Enable request body logging for debugging
#         client_body_buffer_size 1M;
#         client_max_body_size 50M;
        
#         # Root endpoint - health check
#         location = / {
#           add_header Content-Type application/json;
#           return 200 '{"status": "Ollama proxy running", "version": "0.1.48"}';
#         }
        
#         # List models endpoint - this is what HA checks first!
#         location = /api/tags {
#           proxy_pass http://exo_backend/v1/models;
#           proxy_set_header Host $host;
#           proxy_set_header Accept application/json;
          
#           # Buffer the response so we can modify it
#           proxy_buffering on;
#           proxy_buffer_size 4k;
#           proxy_buffers 8 4k;
          
#           # Transform OpenAI response to Ollama format
#           sub_filter_types application/json text/plain;
#           sub_filter_once off;
          
#           # Transform the JSON structure
#           sub_filter '{"object":"list","data":[' '{"models":[';
#           sub_filter '"id":"' '"name":"';
#           sub_filter '"created":' '"modified_at":';
#           sub_filter '"owned_by":"' '"details":{"format":"gguf","family":"';
#           sub_filter '"}' '","parameter_size":"8B","quantization_level":"Q4_0"}}';
#         }
        
#         # Alternative tags endpoint
#         location = /api/tags/ {
#           proxy_pass http://exo_backend/v1/models;
#           proxy_set_header Host $host;
#           proxy_set_header Accept application/json;
          
#           sub_filter_types application/json text/plain;
#           sub_filter_once off;
#           sub_filter '{"object":"list","data":[' '{"models":[';
#           sub_filter '"id":"' '"name":"';
#         }
        
#         # Generate endpoint
#         location = /api/generate {
#           if ($request_method = OPTIONS) {
#             add_header Access-Control-Allow-Origin *;
#             add_header Access-Control-Allow-Methods "GET, POST, OPTIONS";
#             add_header Access-Control-Allow-Headers "Authorization, Content-Type";
#             return 204;
#           }
          
#           proxy_pass http://exo_backend/v1/completions;
#           proxy_method POST;
#           proxy_set_header Host $host;
#           proxy_set_header Content-Type application/json;
#           proxy_read_timeout 300s;
          
#           # Log the request body for debugging
#           access_log /dev/stdout detailed;
#         }
        
#         # Chat endpoint
#         location = /api/chat {
#           if ($request_method = OPTIONS) {
#             add_header Access-Control-Allow-Origin *;
#             add_header Access-Control-Allow-Methods "GET, POST, OPTIONS";
#             add_header Access-Control-Allow-Headers "Authorization, Content-Type";
#             return 204;
#           }
          
#           proxy_pass http://exo_backend/v1/chat/completions;
#           proxy_method POST;
#           proxy_set_header Host $host;
#           proxy_set_header Content-Type application/json;
#           proxy_read_timeout 300s;
#         }
        
#         # Version endpoint - HA might check this
#         location = /api/version {
#           add_header Content-Type application/json;
#           return 200 '{"version":"0.1.48"}';
#         }
        
#         # Show endpoint - returns model details
#         location ~ ^/api/show {
#           add_header Content-Type application/json;
#           return 200 '{"license":"Apache 2.0","modelfile":"FROM llama3","parameters":"temperature 0.7\nstop [INST]","template":"{{ .Prompt }}","details":{"format":"gguf","family":"llama","families":["llama"],"parameter_size":"8B","quantization_level":"Q4_0"}}';
#         }
        
#         # Pull endpoint - fake success
#         location ~ ^/api/pull {
#           add_header Content-Type application/json;
#           return 200 '{"status":"success","digest":"sha256:dummy","total":100,"completed":100}';
#         }
        
#         # Copy endpoint
#         location = /api/copy {
#           add_header Content-Type application/json;
#           return 200 '{"status":"success"}';
#         }
        
#         # Delete endpoint
#         location = /api/delete {
#           add_header Content-Type application/json;
#           return 200 '{"status":"success"}';
#         }
        
#         # List running models
#         location = /api/ps {
#           add_header Content-Type application/json;
#           return 200 '{"models":[]}';
#         }
        
#         # Embeddings endpoint
#         location = /api/embeddings {
#           proxy_pass http://exo_backend/v1/embeddings;
#           proxy_method POST;
#           proxy_set_header Host $host;
#           proxy_set_header Content-Type application/json;
#         }
        
#         # Default handler for any other API calls
#         location /api/ {
#           add_header Content-Type application/json;
#           return 404 '{"error":"Not implemented: $request_uri"}';
#         }
        
#         # CORS headers for all responses
#         add_header Access-Control-Allow-Origin * always;
#         add_header Access-Control-Allow-Methods "GET, POST, OPTIONS" always;
#         add_header Access-Control-Allow-Headers "Authorization, Content-Type" always;
#       }
#     }
# ---
# apiVersion: apps/v1
# kind: Deployment
# metadata:
#   name: ollama-proxy
#   namespace: ollama
#   labels:
#     app: ollama-proxy
# spec:
#   replicas: 1
#   selector:
#     matchLabels:
#       app: ollama-proxy
#   template:
#     metadata:
#       labels:
#         app: ollama-proxy
#     spec:
#       containers:
#         - name: nginx
#           image: nginx:alpine
#           ports:
#             - containerPort: 11434
#               name: ollama-api
#           volumeMounts:
#             - name: nginx-config
#               mountPath: /etc/nginx/nginx.conf
#               subPath: nginx.conf
#           resources:
#             requests:
#               memory: "64Mi"
#               cpu: "50m"
#             limits:
#               memory: "128Mi"
#               cpu: "100m"
#           livenessProbe:
#             httpGet:
#               path: /
#               port: 11434
#             initialDelaySeconds: 10
#             periodSeconds: 30
#           readinessProbe:
#             httpGet:
#               path: /
#               port: 11434
#             initialDelaySeconds: 5
#             periodSeconds: 10
#       volumes:
#         - name: nginx-config
#           configMap:
#             name: ollama-proxy-config
# ---
# apiVersion: v1
# kind: Service
# metadata:
#   name: ollama-proxy
#   namespace: ollama
#   labels:
#     app: ollama-proxy
# spec:
#   ports:
#     - port: 11434
#       targetPort: 11434
#       protocol: TCP
#       name: ollama-api
#   selector:
#     app: ollama-proxy
#   type: ClusterIP
# ---
# apiVersion: v1
# kind: Service
# metadata:
#   name: ollama-proxy-external
#   namespace: ollama
#   labels:
#     app: ollama-proxy
# spec:
#   type: NodePort
#   ports:
#     - port: 11434
#       targetPort: 11434
#       nodePort: 31434
#       protocol: TCP
#       name: ollama-api
#   selector:
#     app: ollama-proxy
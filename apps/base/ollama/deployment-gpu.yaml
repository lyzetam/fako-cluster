# apps/base/ollama/deployment-gpu-complete.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-gpu
  namespace: ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama-gpu
  template:
    metadata:
      labels:
        app: ollama-gpu
    spec:
      # Target pgbee node
      nodeSelector:
        kubernetes.io/hostname: pgbee
      
      containers:
      - name: ollama
        image: ollama/ollama:latest
        ports:
        - containerPort: 11434
          protocol: TCP
        envFrom:
        - configMapRef:
            name: ollama-configmap
        env:
        # Force CPU mode
        - name: CUDA_VISIBLE_DEVICES
          value: ""
        
        # Realistic CPU settings for shared node
        - name: OLLAMA_NUM_PARALLEL
          value: "3"  # 3 concurrent requests
        - name: OLLAMA_NUM_THREAD
          value: "10"  # Use 10 threads
        - name: OLLAMA_MAX_LOADED_MODELS
          value: "3"  # Keep 3 models in memory
        - name: GOMAXPROCS
          value: "10"
        
        # Memory management
        - name: OLLAMA_KEEP_ALIVE
          value: "15m"  # Keep models loaded for 15 minutes
        
        volumeMounts:
        - mountPath: /root/.ollama
          name: ollama-models
        
        resources:
          requests:
            memory: "16Gi"  # Safe with 17GB available
            cpu: "8000m"    # 8 CPU cores requested
          limits:
            memory: "28Gi"  # 28GB limit, leaving room 4GB for other processes
            cpu: "12000m"   # 12 CPU cores max
        
        livenessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
      
      # Model manager - downloads ALL models
      - name: model-manager
        image: ollama/ollama:latest
        command: ["/bin/sh"]
        args:
        - -c
        - |
          echo "Model manager starting..."
          sleep 60
          
          export OLLAMA_HOST=http://localhost:11434
          
          echo ""
          echo "Downloading ALL models - Home Assistant + Full Suite..."
          
          # PRIORITY 1: Home Assistant model
          echo "=== CRITICAL: Home Assistant Model ==="
          ollama pull llama3:2.1b || {
            echo "Failed llama3:2.1b, trying alternatives..."
            ollama pull llama3.2:1b || ollama pull llama3.2:3b
          }
          
          # Small fast models
          echo ""
          echo "=== Small Models (1-4B) ==="
          ollama pull tinyllama || echo "Failed tinyllama"
          ollama pull phi3:mini || echo "Failed phi3:mini"
          # ollama pull phi3:medium-128k-instruct-q5_K_M || echo "Failed phi3:medium"
          # ollama pull gemma2:2b || echo "Failed gemma2:2b"
          # ollama pull llama3.2:1b || echo "Failed llama3.2:1b"
          
          # Medium models (7-8B)
          echo ""
          echo "=== Medium Models (7-8B) ==="
          # ollama pull llama3.1:8b-instruct-q5_K_M || echo "Failed llama3.1:8b"
          # ollama pull mistral:7b-instruct-v0.3-q5_K_M || echo "Failed mistral:7b"
          # ollama pull qwen2.5:7b-instruct-q5_K_M || echo "Failed qwen2.5:7b-q5"
          # ollama pull qwen2.5:7b-instruct-q4_K_M || echo "Failed qwen2.5:7b-q4"
          ollama pull llama3.3:latest || echo "Failed llama3.3 70B"
          ollama pull llama3.2:latest || echo "Failed llama3.2"
          
          
          # Specialized models
          echo ""
          echo "=== Specialized Models ==="
          # ollama pull codellama:7b-instruct-q5_K_M || echo "Failed codellama:7b"
          # ollama pull codellama:13b-instruct-q4_K_M || echo "Failed codellama:13b"
          
          # Larger models
          echo ""
          echo "=== Larger Models (13B+) ==="
          # ollama pull llama2:13b-chat-q4_K_M || echo "Failed llama2:13b"
          ollama pull deepseek-r1:14b || echo "Failed deepseek-r1:14b"
          ollama pull phi-4:latest || echo "Failed phi-4"
          
          # Optional very large model
          # echo ""
          # echo "=== Optional Large Model ==="
          # ollama pull mixtral:8x7b-instruct-v0.1-q2_K || echo "Failed mixtral (expected)"
          
          echo ""
          echo "Model download complete. Available models:"
          ollama list
          
          while true; do
            sleep 600
            echo "Model manager heartbeat..."
          done
        
        volumeMounts:
        - mountPath: /root/.ollama
          name: ollama-models
        resources:
          requests:
            memory: "1Gi"  
            cpu: "500m"     
          limits:
            memory: "2Gi"
            cpu: "1000m"
      
      volumes:
      - name: ollama-models
        persistentVolumeClaim:
          claimName: ollama-models


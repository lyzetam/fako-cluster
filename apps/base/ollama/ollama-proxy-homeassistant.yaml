# apps/base/ollama/ollama-proxy-homeassistant.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-proxy-ha
  namespace: ollama
data:
  server.js: |
    const http = require('http');
    const url = require('url');
    
    const EXO_HOST = '***NFS-IP-REMOVED***';
    const EXO_PORT = 52415;
    const PROXY_PORT = 11434;
    
    console.log(`Starting Ollama proxy for Home Assistant...`);
    console.log(`Proxying to exo cluster at ${EXO_HOST}:${EXO_PORT}`);
    
    // Helper to make HTTP requests
    function makeRequest(options, postData = null) {
      return new Promise((resolve, reject) => {
        console.log(`Making request to ${options.hostname}:${options.port}${options.path}`);
        
        const req = http.request(options, (res) => {
          let data = '';
          res.on('data', chunk => data += chunk);
          res.on('end', () => {
            console.log(`Response status: ${res.statusCode}`);
            try {
              const parsed = JSON.parse(data);
              resolve({ status: res.statusCode, data: parsed });
            } catch (e) {
              // Return raw data if not JSON
              resolve({ status: res.statusCode, data: data });
            }
          });
        });
        
        req.on('error', (err) => {
          console.error('Request error:', err);
          reject(err);
        });
        
        if (postData) {
          console.log('Request body:', postData);
          req.write(postData);
        }
        req.end();
      });
    }
    
    const server = http.createServer(async (req, res) => {
      console.log(`\n${new Date().toISOString()} - ${req.method} ${req.url}`);
      
      // CORS headers for all requests
      res.setHeader('Access-Control-Allow-Origin', '*');
      res.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS');
      res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization');
      
      // Handle OPTIONS preflight
      if (req.method === 'OPTIONS') {
        res.writeHead(204);
        res.end();
        return;
      }
      
      const parsedUrl = url.parse(req.url, true);
      
      try {
        // Root endpoint - health check
        if (parsedUrl.pathname === '/' || parsedUrl.pathname === '/health') {
          res.writeHead(200, { 'Content-Type': 'application/json' });
          res.end(JSON.stringify({ 
            status: 'ok',
            message: 'Ollama proxy for Home Assistant',
            version: '0.1.48',
            exo_backend: `${EXO_HOST}:${EXO_PORT}`
          }));
          return;
        }
        
        // Version endpoint - Home Assistant checks this
        if (parsedUrl.pathname === '/api/version') {
          res.writeHead(200, { 'Content-Type': 'application/json' });
          res.end(JSON.stringify({ 
            version: '0.1.48',
            api_version: '0.1.48'
          }));
          return;
        }
        
        // List models - CRITICAL for Home Assistant
        if (parsedUrl.pathname === '/api/tags' || parsedUrl.pathname === '/api/tags/') {
          console.log('Fetching models from exo...');
          
          try {
            const result = await makeRequest({
              hostname: EXO_HOST,
              port: EXO_PORT,
              path: '/v1/models',
              method: 'GET',
              headers: {
                'Accept': 'application/json'
              }
            });
            
            console.log('Raw exo response:', JSON.stringify(result.data, null, 2));
            
            // Transform OpenAI format to Ollama format
            const models = result.data.data || [];
            const ollamaResponse = {
              models: models.map(model => ({
                name: model.id,
                model: model.id,
                modified_at: new Date().toISOString(),
                size: 4000000000, // 4GB estimate
                digest: `sha256:${Buffer.from(model.id).toString('hex').substring(0, 12)}`,
                details: {
                  parent_model: '',
                  format: 'gguf',
                  family: model.id.split('-')[0] || 'llama',
                  families: [model.id.split('-')[0] || 'llama'],
                  parameter_size: '8B',
                  quantization_level: 'Q4_0'
                }
              }))
            };
            
            console.log('Transformed response:', JSON.stringify(ollamaResponse, null, 2));
            
            res.writeHead(200, { 'Content-Type': 'application/json' });
            res.end(JSON.stringify(ollamaResponse));
          } catch (error) {
            console.error('Error fetching models:', error);
            
            // Return a default model if exo is unreachable
            const fallbackResponse = {
              models: [{
                name: 'llama-3.1-8b',
                model: 'llama-3.1-8b',
                modified_at: new Date().toISOString(),
                size: 4000000000,
                digest: 'sha256:defaultdigest',
                details: {
                  parent_model: '',
                  format: 'gguf',
                  family: 'llama',
                  families: ['llama'],
                  parameter_size: '8B',
                  quantization_level: 'Q4_0'
                }
              }]
            };
            
            res.writeHead(200, { 'Content-Type': 'application/json' });
            res.end(JSON.stringify(fallbackResponse));
          }
          return;
        }
        
        // Show model details
        if (parsedUrl.pathname.match(/^\/api\/show/)) {
          const modelName = parsedUrl.pathname.split('/').pop() || 'llama-3.1-8b';
          
          res.writeHead(200, { 'Content-Type': 'application/json' });
          res.end(JSON.stringify({
            license: 'Apache 2.0',
            modelfile: `FROM ${modelName}`,
            parameters: 'temperature 0.7\nstop [INST]',
            template: '{{ .Prompt }}',
            model: modelName,
            modified_at: new Date().toISOString(),
            digest: `sha256:${Buffer.from(modelName).toString('hex').substring(0, 12)}`,
            details: {
              parent_model: '',
              format: 'gguf',
              family: modelName.split('-')[0] || 'llama',
              families: [modelName.split('-')[0] || 'llama'],
              parameter_size: '8B',
              quantization_level: 'Q4_0'
            }
          }));
          return;
        }
        
        // Generate endpoint - for completions
        if (parsedUrl.pathname === '/api/generate' && req.method === 'POST') {
          let body = '';
          req.on('data', chunk => body += chunk);
          req.on('end', async () => {
            try {
              const requestData = JSON.parse(body);
              console.log('Generate request:', requestData);
              
              const openAiRequest = {
                model: requestData.model || 'llama-3.1-8b',
                prompt: requestData.prompt,
                max_tokens: requestData.options?.num_predict || 1000,
                temperature: requestData.options?.temperature || 0.7,
                stream: requestData.stream || false
              };
              
              const result = await makeRequest({
                hostname: EXO_HOST,
                port: EXO_PORT,
                path: '/v1/completions',
                method: 'POST',
                headers: { 
                  'Content-Type': 'application/json',
                  'Authorization': 'Bearer dummy'
                }
              }, JSON.stringify(openAiRequest));
              
              // Transform response to Ollama format
              const ollamaResponse = {
                model: requestData.model,
                created_at: new Date().toISOString(),
                response: result.data.choices?.[0]?.text || '',
                done: true,
                context: requestData.context || [],
                total_duration: 1000000000,
                load_duration: 100000000,
                prompt_eval_count: requestData.prompt.length,
                prompt_eval_duration: 100000000,
                eval_count: result.data.choices?.[0]?.text?.length || 0,
                eval_duration: 800000000
              };
              
              res.writeHead(200, { 'Content-Type': 'application/json' });
              res.end(JSON.stringify(ollamaResponse));
            } catch (error) {
              console.error('Error in generate:', error);
              res.writeHead(500, { 'Content-Type': 'application/json' });
              res.end(JSON.stringify({ 
                error: 'Generation failed', 
                details: error.message 
              }));
            }
          });
          return;
        }
        
        // Chat endpoint - for conversations
        if (parsedUrl.pathname === '/api/chat' && req.method === 'POST') {
          let body = '';
          req.on('data', chunk => body += chunk);
          req.on('end', async () => {
            try {
              const requestData = JSON.parse(body);
              console.log('Chat request:', requestData);
              
              // Transform Ollama messages to OpenAI format
              const messages = requestData.messages || [];
              
              const openAiRequest = {
                model: requestData.model || 'llama-3.1-8b',
                messages: messages,
                temperature: requestData.options?.temperature || 0.7,
                max_tokens: requestData.options?.num_predict || 1000,
                stream: requestData.stream || false
              };
              
              const result = await makeRequest({
                hostname: EXO_HOST,
                port: EXO_PORT,
                path: '/v1/chat/completions',
                method: 'POST',
                headers: { 
                  'Content-Type': 'application/json',
                  'Authorization': 'Bearer dummy'
                }
              }, JSON.stringify(openAiRequest));
              
              // Transform response to Ollama format
              const message = result.data.choices?.[0]?.message || { role: 'assistant', content: '' };
              
              const ollamaResponse = {
                model: requestData.model,
                created_at: new Date().toISOString(),
                message: {
                  role: message.role,
                  content: message.content
                },
                done: true,
                done_reason: 'stop',
                total_duration: 1000000000,
                load_duration: 100000000,
                prompt_eval_count: 100,
                prompt_eval_duration: 100000000,
                eval_count: message.content?.length || 0,
                eval_duration: 800000000
              };
              
              res.writeHead(200, { 'Content-Type': 'application/json' });
              res.end(JSON.stringify(ollamaResponse));
            } catch (error) {
              console.error('Error in chat:', error);
              res.writeHead(500, { 'Content-Type': 'application/json' });
              res.end(JSON.stringify({ 
                error: 'Chat failed', 
                details: error.message 
              }));
            }
          });
          return;
        }
        
        // Pull endpoint - fake success for Home Assistant
        if (parsedUrl.pathname.match(/^\/api\/pull/)) {
          res.writeHead(200, { 'Content-Type': 'application/json' });
          res.end(JSON.stringify({
            status: 'success',
            digest: 'sha256:dummy',
            total: 100,
            completed: 100
          }));
          return;
        }
        
        // List running models
        if (parsedUrl.pathname === '/api/ps') {
          res.writeHead(200, { 'Content-Type': 'application/json' });
          res.end(JSON.stringify({
            models: [{
              name: 'llama-3.1-8b',
              model: 'llama-3.1-8b',
              size: 4000000000,
              digest: 'sha256:dummy',
              details: {
                format: 'gguf',
                family: 'llama'
              }
            }]
          }));
          return;
        }
        
        // Default 404 for unhandled routes
        console.log(`Unhandled route: ${parsedUrl.pathname}`);
        res.writeHead(404, { 'Content-Type': 'application/json' });
        res.end(JSON.stringify({ 
          error: 'Not found', 
          path: parsedUrl.pathname,
          message: 'This Ollama endpoint is not implemented'
        }));
        
      } catch (error) {
        console.error('Server error:', error);
        res.writeHead(500, { 'Content-Type': 'application/json' });
        res.end(JSON.stringify({ 
          error: 'Internal server error', 
          details: error.message 
        }));
      }
    });
    
    server.listen(PROXY_PORT, '0.0.0.0', () => {
      console.log(`Ollama proxy listening on 0.0.0.0:${PROXY_PORT}`);
      console.log(`Ready to serve Home Assistant requests`);
    });
    
    // Graceful shutdown
    process.on('SIGTERM', () => {
      console.log('Received SIGTERM, shutting down gracefully...');
      server.close(() => {
        console.log('Server closed');
        process.exit(0);
      });
    });
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-proxy-ha
  namespace: ollama
  labels:
    app: ollama-proxy-ha
    component: proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama-proxy-ha
  template:
    metadata:
      labels:
        app: ollama-proxy-ha
        component: proxy
    spec:
      containers:
        - name: proxy
          image: node:20-alpine
          command: ["node", "/app/server.js"]
          ports:
            - containerPort: 11434
              name: ollama-api
              protocol: TCP
          volumeMounts:
            - name: app
              mountPath: /app
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"
          env:
            - name: NODE_ENV
              value: "production"
          livenessProbe:
            httpGet:
              path: /health
              port: 11434
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /api/version
              port: 11434
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
          startupProbe:
            httpGet:
              path: /api/tags
              port: 11434
            initialDelaySeconds: 5
            periodSeconds: 5
            failureThreshold: 10
      volumes:
        - name: app
          configMap:
            name: ollama-proxy-ha
---
apiVersion: v1
kind: Service
metadata:
  name: ollama-ha
  namespace: ollama
  labels:
    app: ollama-proxy-ha
    component: proxy
spec:
  ports:
    - port: 11434
      targetPort: 11434
      protocol: TCP
      name: ollama-api
  selector:
    app: ollama-proxy-ha
  type: ClusterIP
---
# NodePort service for Home Assistant access
apiVersion: v1
kind: Service
metadata:
  name: ollama-ha-nodeport
  namespace: ollama
  labels:
    app: ollama-proxy-ha
    component: external-access
spec:
  type: NodePort
  ports:
    - port: 11434
      targetPort: 11434
      nodePort: 31434  # Home Assistant will connect to any-node-ip:31434
      protocol: TCP
      name: ollama-api
  selector:
    app: ollama-proxy-ha
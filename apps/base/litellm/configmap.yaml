# apps/base/litellm/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-run-script
  namespace: litellm
data:
  run.py: |
    import os
    import subprocess
    import sys
    import time
    
    # Install specific version of litellm with proxy dependencies and database tools
    print("Installing LiteLLM with proxy dependencies...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "litellm[proxy]==1.35.0", "psycopg2-binary", "prisma"])
    
    # Set environment variables
    os.environ["OPENAI_API_KEY"] = "dummy"
    os.environ["LITELLM_MASTER_KEY"] = "sk-1022"
    os.environ["DATABASE_URL"] = "postgresql://litellm:litellm-secret-password@postgres-service:5432/litellm"
    os.environ["STORE_MODEL_IN_DB"] = "True"
    os.environ["UI_USERNAME"] = "admin"
    os.environ["UI_PASSWORD"] = "admin123"
    
    # Wait for database to be ready
    print("Waiting for database to be ready...")
    time.sleep(15)
    
    # Generate Prisma client
    print("Generating Prisma client...")
    try:
        # Find the litellm package location
        import litellm
        litellm_path = os.path.dirname(litellm.__file__)
        schema_path = os.path.join(litellm_path, "proxy", "schema.prisma")
        
        # Set PRISMA_PY_DEBUG_GENERATOR for debugging if needed
        os.environ["PRISMA_PY_DEBUG_GENERATOR"] = "1"
        
        # Generate the Prisma client
        subprocess.check_call([sys.executable, "-m", "prisma", "generate", "--schema", schema_path])
        print("Prisma client generated successfully!")
    except Exception as e:
        print(f"Warning: Failed to generate Prisma client: {e}")
        print("Continuing anyway...")
    
    # Start litellm with correct arguments and config
    print("Starting LiteLLM...")
    subprocess.run([
        "litellm",
        "--model", "openai/llama-3.1-8b",
        "--api_base", "http://***NFS-IP-REMOVED***:52415/v1",
        "--port", "4000",
        "--telemetry", "False"
    ])

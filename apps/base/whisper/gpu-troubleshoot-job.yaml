# apps/base/whisper/gpu-troubleshoot-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: whisper-gpu-troubleshoot
  namespace: whisper
  labels:
    app: whisper-gpu-troubleshoot
spec:
  ttlSecondsAfterFinished: 600
  backoffLimit: 0
  template:
    spec:
      runtimeClassName: nvidia
      nodeSelector:
        kubernetes.io/hostname: playground
        nvidia.com/gpu: "true"
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      restartPolicy: Never
      
      containers:
      - name: troubleshoot
        image: nvidia/cuda:12.2.0-cudnn8-devel-ubuntu22.04
        command:
        - /bin/bash
        - -c
        - |
          set -e
          echo "=== Whisper GPU Troubleshooting ==="
          echo "Date: $(date)"
          echo "Node: $(hostname)"
          echo ""
          
          # Check GPU
          echo "1. GPU Information:"
          nvidia-smi || echo "nvidia-smi failed"
          echo ""
          
          # Check CUDA
          echo "2. CUDA Version:"
          nvcc --version || echo "nvcc not found"
          echo ""
          
          # Check libraries
          echo "3. CUDA Libraries:"
          ldconfig -p | grep cuda || echo "No CUDA libraries in ldconfig"
          echo ""
          
          echo "4. cuDNN check:"
          ls -la /usr/lib/x86_64-linux-gnu/libcudnn* 2>/dev/null || echo "No cuDNN found in /usr/lib"
          ls -la /usr/local/cuda/lib64/libcudnn* 2>/dev/null || echo "No cuDNN found in /usr/local/cuda"
          echo ""
          
          # Install Python and test
          echo "5. Installing Python and testing GPU..."
          apt-get update -qq
          apt-get install -y python3 python3-pip
          pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
          
          python3 -c "
          import torch
          print('PyTorch version:', torch.__version__)
          print('CUDA available:', torch.cuda.is_available())
          if torch.cuda.is_available():
              print('CUDA version:', torch.version.cuda)
              print('GPU name:', torch.cuda.get_device_name(0))
              print('GPU memory:', torch.cuda.get_device_properties(0).total_memory / 1024**3, 'GB')
              
              # Test computation
              x = torch.rand(1000, 1000).cuda()
              y = torch.rand(1000, 1000).cuda()
              z = torch.matmul(x, y)
              print('GPU computation test: PASSED')
          "
          
          # Test Whisper
          echo ""
          echo "6. Testing Whisper with GPU..."
          pip3 install faster-whisper
          
          python3 -c "
          try:
              from faster_whisper import WhisperModel
              print('Loading Whisper model...')
              model = WhisperModel('tiny', device='cuda', compute_type='float16')
              print('Whisper GPU test: PASSED')
          except Exception as e:
              print(f'Whisper GPU test FAILED: {e}')
          "
          
          echo ""
          echo "=== Troubleshooting Complete ==="
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
            nvidia.com/gpu: 1
          limits:
            memory: "8Gi"
            cpu: "4000m"
            nvidia.com/gpu: 1
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"
          
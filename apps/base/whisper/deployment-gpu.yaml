# apps/base/whisper/deployment-gpu.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: whisper-gpu
  namespace: whisper
spec:
  replicas: 1
  selector:
    matchLabels:
      app: whisper-gpu
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: whisper-gpu
    spec:
      runtimeClassName: nvidia
      nodeSelector:
        kubernetes.io/hostname: playground
        nvidia.com/gpu: "true"
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      priorityClassName: system-cluster-critical
      
      containers:
      # Main Whisper container
      - name: whisper
        image: rhasspy/wyoming-whisper:latest
        ports:
        - containerPort: 10300
          protocol: TCP
        args:
        - "--model"
        - "small"
        - "--uri"
        - "tcp://0.0.0.0:10300"
        - "--data-dir"
        - "/data"
        - "--download-dir"
        - "/data"
        - "--device"
        - "cuda"
        - "--compute-type"
        - "float16"
        volumeMounts:
        - mountPath: /data
          name: whisper-models
        - mountPath: /usr/local/cuda-libs
          name: cuda-libs
        resources:
          requests:
            memory: "4Gi"
            cpu: "1000m"
            nvidia.com/gpu: 1
          limits:
            memory: "8Gi"
            cpu: "2000m"
            nvidia.com/gpu: 1
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"
        - name: LD_LIBRARY_PATH
          value: "/usr/local/cuda-libs/lib64:/usr/local/cuda-libs/lib:$LD_LIBRARY_PATH"
        livenessProbe:
          tcpSocket:
            port: 10300
          initialDelaySeconds: 180
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 5
        readinessProbe:
          tcpSocket:
            port: 10300
          initialDelaySeconds: 120
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 10
      
      # CUDA library provider sidecar
      - name: cuda-provider
        image: nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04
        command:
        - /bin/bash
        - -c
        - |
          echo "CUDA Library Provider Started"
          echo "Providing CUDA libraries to shared volume..."
          
          # Copy CUDA libraries to shared volume
          cp -r /usr/local/cuda/* /cuda-libs/ 2>/dev/null || true
          cp -r /usr/lib/x86_64-linux-gnu/libcudnn* /cuda-libs/lib64/ 2>/dev/null || true
          cp -r /usr/lib/x86_64-linux-gnu/libcublas* /cuda-libs/lib64/ 2>/dev/null || true
          cp -r /usr/lib/x86_64-linux-gnu/libnccl* /cuda-libs/lib64/ 2>/dev/null || true
          
          echo "CUDA libraries copied. Available libraries:"
          ls -la /cuda-libs/lib64/ | grep -E "(cuda|cudnn|cublas)" | head -20
          
          # Keep container running
          while true; do
            sleep 3600
            echo "CUDA provider heartbeat at $(date)"
          done
        volumeMounts:
        - mountPath: /cuda-libs
          name: cuda-libs
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
      
      volumes:
      - name: whisper-models
        persistentVolumeClaim:
          claimName: whisper-models
      - name: cuda-libs
        emptyDir:
          sizeLimit: 5Gi
# Fixed Alloy configuration with logs, metrics, and traces
apiVersion: v1
kind: ConfigMap
metadata:
  name: alloy-config
  namespace: alloy
data:
  config.alloy: |
    // ============================================
    // Logging configuration
    // ============================================
    logging {
      level = "info"
      format = "logfmt"
    }
    
    // ============================================
    // Kubernetes Pod Logs Collection
    // ============================================
    
    discovery.kubernetes "pods" {
      role = "pod"
    }
    
    discovery.relabel "pod_logs" {
      targets = discovery.kubernetes.pods.targets
      
      // Only keep running pods
      rule {
        source_labels = ["__meta_kubernetes_pod_phase"]
        regex = "Running"
        action = "keep"
      }
      
      // Set namespace label
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        action = "replace"
        target_label = "namespace"
      }
      
      // Set pod name
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        action = "replace"
        target_label = "pod"
      }
      
      // Set container name
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        action = "replace"
        target_label = "container"
      }
      
      // Set app label - improved logic
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
        action = "replace"
        target_label = "app"
      }
      
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app"]
        regex = "(.+)"
        action = "replace"
        target_label = "app"
      }
      
      // Set node name
      rule {
        source_labels = ["__meta_kubernetes_pod_node_name"]
        action = "replace"
        target_label = "node"
      }
      
      // Set job label
      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
        action = "replace"
        target_label = "job"
        separator = "/"
        replacement = "$1"
      }
      
      // Set __path__ for log files
      rule {
        source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
        action = "replace"
        target_label = "__path__"
        separator = "/"
        replacement = "/var/log/pods/*$1/*.log"
      }
      
      // Copy all pod labels
      rule {
        action = "labelmap"
        regex = "__meta_kubernetes_pod_label_(.+)"
      }
    }
    
    // Kubernetes log source with better error handling
    loki.source.kubernetes "pods" {
      targets = discovery.relabel.pod_logs.output
      forward_to = [loki.process.cri_parser.receiver]
    }
    
    // CRI log parser with better stage management
    loki.process "cri_parser" {
      forward_to = [loki.process.app_specific.receiver]
      
      stage.cri {}
      
      // Add cluster label
      stage.static_labels {
        values = {
          cluster = "production",
        }
      }
      
      // Drop unwanted labels
      stage.label_drop {
        values = ["filename", "stream"]
      }
    }
    
    // App-specific parsing with more examples
    loki.process "app_specific" {
      forward_to = [loki.write.default.receiver]
      
      // Nginx log parsing
      stage.match {
        selector = "{app=\"nginx\"}"
        
        stage.regex {
          expression = "(?P<remote_addr>[\\w\\.]+) - (?P<remote_user>[^ ]*) \\[(?P<time_local>[^\\]]+)\\] \"(?P<method>[^ ]*) (?P<request>[^ ]*) (?P<protocol>[^ ]*)\" (?P<status>[\\d]+) (?P<body_bytes_sent>[\\d]+)"
        }
        
        stage.labels {
          values = {
            method = "",
            status = "",
          }
        }
      }
      
      // JSON parsing for structured logs
      stage.match {
        selector = "{app=~\"api|backend|frontend\"}"
        
        stage.json {
          expressions = {
            level = "level",
            msg = "msg",
            trace_id = "trace_id",
            span_id = "span_id",
          }
        }
        
        stage.labels {
          values = {
            level = "",
            trace_id = "",
          }
        }
      }
    }
    
    // ============================================
    // Kubernetes Events Collection
    // ============================================
    
    loki.source.kubernetes_events "events" {
      job_name = "kubernetes-events"
      log_format = "logfmt"
      forward_to = [loki.process.events.receiver]
    }
    
    loki.process "events" {
      forward_to = [loki.write.default.receiver]
      
      stage.static_labels {
        values = {
          job = "kubernetes-events",
          cluster = "production",
        }
      }
    }
    
    // ============================================
    // System Logs Collection
    // ============================================
    
    local.file_match "system_logs" {
      path_targets = [
        {__path__ = "/var/log/syslog", job = "syslog"},
        {__path__ = "/var/log/messages", job = "messages"},
        {__path__ = "/var/log/secure", job = "secure"},
        {__path__ = "/var/log/auth.log", job = "auth"},
      ]
    }
    
    loki.source.file "system" {
      targets = local.file_match.system_logs.targets
      forward_to = [loki.process.system.receiver]
      tail_from_end = false
    }
    
    loki.process "system" {
      forward_to = [loki.write.default.receiver]
      
      stage.static_labels {
        values = {
          cluster = "production",
          node = env("HOSTNAME"),
        }
      }
    }
    
    // ============================================
    // Journal Logs Collection
    // ============================================
    
    loki.source.journal "systemd" {
      forward_to = [loki.process.journal_labels.receiver]
      path = "/var/log/journal"
      max_age = "12h"
      labels = {
        job = "systemd-journal",
      }
    }
    
    loki.process "journal_labels" {
      forward_to = [loki.write.default.receiver]
      
      stage.labels {
        values = {
          unit = "__journal__systemd_unit",
          hostname = "__journal__hostname",
          priority = "__journal_priority",
        }
      }
      
      stage.static_labels {
        values = {
          cluster = "production",
        }
      }
      
      stage.label_drop {
        values = ["__journal__systemd_unit", "__journal__hostname", "__journal_priority"]
      }
    }
    
    // ============================================
    // Loki Write Configuration
    // ============================================
    
    loki.write "default" {
      endpoint {
        url = "http://loki-gateway.loki-stack.svc.cluster.local/loki/api/v1/push"
        
        // Batch configuration for better performance
        batch_size = 1048576  // 1MB
        batch_wait = "1s"
        
        // Retry configuration
        min_backoff = "500ms"
        max_backoff = "5m"
        max_retries = 10
      }
      
      external_labels = {
        cluster = "production",
        environment = "staging",
      }
    }
    
    // ============================================
    // Metrics Collection
    // ============================================
    
    // Node metrics
    prometheus.exporter.unix "node_metrics" {
      set_collectors = ["cpu", "meminfo", "filesystem", "netdev", "diskstats", "loadavg", "time", "vmstat", "netstat"]
      disable_collectors = ["wifi", "thermal"]
      
      filesystem {
        fs_types_exclude = "^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$"
        mount_points_exclude = "^/(dev|proc|sys|var/lib/docker/.+|run/containerd/.+)($|/)"
      }
    }
    
    prometheus.scrape "node_exporter" {
      targets = prometheus.exporter.unix.node_metrics.targets
      forward_to = [prometheus.relabel.node_metrics.receiver]
      scrape_interval = "15s"
      scrape_timeout = "10s"
    }
    
    prometheus.relabel "node_metrics" {
      forward_to = [prometheus.remote_write.default.receiver]
      
      rule {
        target_label = "job"
        replacement = "node-exporter"
      }
      
      rule {
        target_label = "instance"
        replacement = env("HOSTNAME")
      }
    }
    
    // Kubernetes metrics collection
    prometheus.exporter.kubernetes "cluster_metrics" {
      // This collects kube-state-metrics style metrics
    }
    
    prometheus.scrape "kubernetes_metrics" {
      targets = prometheus.exporter.kubernetes.cluster_metrics.targets
      forward_to = [prometheus.remote_write.default.receiver]
      scrape_interval = "30s"
    }
    
    // ServiceMonitor discovery - expanded namespaces
    prometheus.operator.servicemonitors "service_monitors" {
      forward_to = [prometheus.remote_write.default.receiver]
      
      // Include all relevant namespaces
      namespaces = ["gpu-monitoring", "monitoring", "gpu-operator", "default", "alloy", "loki-stack"]
      
      // Optionally scrape all namespaces
      // namespaces = []
    }
    
    // PodMonitor discovery
    prometheus.operator.podmonitors "pod_monitors" {
      forward_to = [prometheus.remote_write.default.receiver]
      namespaces = ["gpu-monitoring", "monitoring", "default"]
    }
    
    // Scrape cAdvisor metrics
    discovery.kubernetes "cadvisor" {
      role = "node"
    }
    
    prometheus.scrape "cadvisor" {
      targets = discovery.kubernetes.cadvisor.targets
      forward_to = [prometheus.remote_write.default.receiver]
      scheme = "https"
      scrape_interval = "30s"
      metrics_path = "/metrics/cadvisor"
      
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      
      tls_config {
        ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
        insecure_skip_verify = true
      }
    }
    
    // Remote write to Prometheus
    prometheus.remote_write "default" {
      endpoint {
        url = "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090/api/v1/write"
        
        // Queue configuration
        queue_config {
          capacity = 50000
          max_shards = 50
          min_shards = 1
          max_samples_per_send = 5000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
        }
        
        // Metadata configuration
        metadata_config {
          send = true
          send_interval = "1m"
        }
      }
      
      external_labels = {
        cluster = "production",
        alloy_instance = env("HOSTNAME"),
      }
    }
    
    // ============================================
    // Traces Collection (OpenTelemetry)
    // ============================================
    
    // OTLP receiver for traces
    otelcol.receiver.otlp "default" {
      grpc {
        endpoint = "0.0.0.0:4317"
      }
      
      http {
        endpoint = "0.0.0.0:4318"
      }
      
      output {
        traces = [otelcol.processor.batch.default.input]
      }
    }
    
    // Batch processor for traces
    otelcol.processor.batch "default" {
      timeout = "2s"
      send_batch_size = 1000
      
      output {
        traces = [otelcol.exporter.otlp.tempo.input]
      }
    }
    
    // Export traces to Tempo (if you have it) or Jaeger
    otelcol.exporter.otlp "tempo" {
      client {
        endpoint = "tempo-distributor.tempo.svc.cluster.local:4317"
        
        tls {
          insecure = true
          insecure_skip_verify = true
        }
      }
    }
    
    // ============================================
    // Self-monitoring
    // ============================================
    
    prometheus.exporter.self "alloy" {}
    
    prometheus.scrape "self" {
      targets = prometheus.exporter.self.alloy.targets
      forward_to = [prometheus.remote_write.default.receiver]
      scrape_interval = "30s"
      
      // Add job label
      job_name = "alloy"
    }